{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5ej_f6s0BcQ",
        "outputId": "c9156780-6229-4e24-dcf6-a397dcf5be80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "urKb9Iom0EsU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Literal, Dict, Any, Optional\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    PrefixTuningConfig,\n",
        "    TaskType,\n",
        ")\n",
        "\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sE6TGmRFxE4"
      },
      "source": [
        "# load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyJC3Dg6FyQc",
        "outputId": "0ccd0d4f-6eb9-491e-b80c-efa130a7aa93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SST2 small train: 8000\n",
            "MRPC small train: 800\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "sst2 = load_dataset(\"glue\", \"sst2\")\n",
        "mrpc = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "# ====== ‚ö° Â≠êÈááÊ†∑ÔºåËÆ©ËÆ≠ÁªÉÂ§ßÂπÖÂä†ÈÄüÔºà‰Ω†ÂèØ‰ª•Ë∞ÉÂ§ßÂ∞èÔºâ ======\n",
        "SST2_TRAIN = 8000\n",
        "SST2_VAL   = 300\n",
        "MRPC_TRAIN = 800\n",
        "MRPC_VAL   = 200\n",
        "\n",
        "sst2_small = {\n",
        "    \"train\": sst2[\"train\"].select(range(SST2_TRAIN)),\n",
        "    \"validation\": sst2[\"validation\"].select(range(SST2_VAL))\n",
        "}\n",
        "\n",
        "mrpc_small = {\n",
        "    \"train\": mrpc[\"train\"].select(range(MRPC_TRAIN)),\n",
        "    \"validation\": mrpc[\"validation\"].select(range(MRPC_VAL))\n",
        "}\n",
        "\n",
        "raw_datasets = {\n",
        "    \"sst2\": sst2_small,\n",
        "    \"mrpc\": mrpc_small,\n",
        "}\n",
        "\n",
        "print(\"SST2 small train:\", len(raw_datasets[\"sst2\"][\"train\"]))\n",
        "print(\"MRPC small train:\", len(raw_datasets[\"mrpc\"][\"train\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyHYa1xe1Q9p"
      },
      "source": [
        "# Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NAlfp4qX1M49"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    learning_rate: float = 2e-5\n",
        "    batch_size: int = 64\n",
        "    num_epochs: int = 3\n",
        "    max_length: int = 128\n",
        "    logging_steps: int = 50\n",
        "    eval_strategy: str = \"epoch\"\n",
        "    save_strategy: str = \"no\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PEFTConfig:\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: float = 0.05\n",
        "    adapter_num_virtual_tokens: int = 20\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    model_name: str\n",
        "    task_name: Literal[\"sst2\", \"mrpc\"]\n",
        "    peft_method: Literal[\"full_ft\", \"bitfit\", \"lora\", \"adapter\", \"prefix\"]\n",
        "    training: TrainingConfig = None\n",
        "    peft: PEFTConfig = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.training = self.training or TrainingConfig()\n",
        "        self.peft = self.peft or PEFTConfig()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OA6xo8p1X2b"
      },
      "source": [
        "# Tokenize And Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "UeeJlKXq1P6o"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "_tokenization_cache = {}\n",
        "\n",
        "def get_cached_tokenization(\n",
        "    model_name: str,\n",
        "    task_name: str,\n",
        "    raw_dataset,\n",
        "    max_length: int = 128,\n",
        "    cache_dir: Path = Path(\"./cache/tokenized\")\n",
        "):\n",
        "    \"\"\"ËøîÂõû (encoded_dataset, tokenizer)ÔºåËá™Âä®ÁºìÂ≠ò„ÄÇ\"\"\"\n",
        "\n",
        "    cache_key = (model_name, task_name, max_length)\n",
        "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    cache_file = cache_dir / f\"{model_name.replace('/', '_')}_{task_name}_{max_length}.pkl\"\n",
        "\n",
        "    # ---------- Memory cache ----------\n",
        "    if cache_key in _tokenization_cache:\n",
        "        print(f\"‚úì Using cached tokenization (memory) for {model_name}/{task_name}\")\n",
        "        return _tokenization_cache[cache_key]\n",
        "\n",
        "    # ---------- Disk cache ----------\n",
        "    if cache_file.exists():\n",
        "        print(f\"‚úì Loading cached tokenization from: {cache_file}\")\n",
        "        with open(cache_file, \"rb\") as f:\n",
        "            encoded_dataset, tokenizer = pickle.load(f)\n",
        "        _tokenization_cache[cache_key] = (encoded_dataset, tokenizer)\n",
        "        return encoded_dataset, tokenizer\n",
        "\n",
        "    # ---------- Compute tokenization ----------\n",
        "    print(f\"‚ö† Tokenizing {model_name}/{task_name} ...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if model_name.startswith(\"t5\"):\n",
        "        tokenizer.eos_token = tokenizer.eos_token or \"</s>\"\n",
        "        tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
        "        tokenizer.padding_side = \"right\"\n",
        "\n",
        "    def preprocess_fn(examples):\n",
        "        return tokenizer(\n",
        "            examples.get(\"sentence1\") or examples.get(\"sentence\"),\n",
        "            examples.get(\"sentence2\"),\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "\n",
        "    columns_to_remove = [\n",
        "        col for col in raw_dataset[\"train\"].column_names\n",
        "        if col not in [\"label\", \"labels\"]\n",
        "    ]\n",
        "\n",
        "    encoded_dataset = raw_dataset.map(\n",
        "        preprocess_fn,\n",
        "        batched=True,\n",
        "        remove_columns=columns_to_remove\n",
        "    )\n",
        "\n",
        "    # ---------- Save cache ----------\n",
        "    with open(cache_file, \"wb\") as f:\n",
        "        pickle.dump((encoded_dataset, tokenizer), f)\n",
        "\n",
        "    _tokenization_cache[cache_key] = (encoded_dataset, tokenizer)\n",
        "    print(f\"‚úì Saved tokenization cache ‚Üí {cache_file}\")\n",
        "\n",
        "    return encoded_dataset, tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWcbS1Vy2jOX"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "brFVQj6v1Pdb"
      },
      "outputs": [],
      "source": [
        "# Bert Models Build\n",
        "def build_bert_fullft(model_name, num_labels=2):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labels\n",
        "    )\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "    return model, total, total\n",
        "\n",
        "\n",
        "def build_bert_bitfit(model_name, num_labels=2):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labels\n",
        "    )\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = 0\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"bias\" in name or \"classifier\" in name:\n",
        "            p.requires_grad = True\n",
        "            trainable += p.numel()\n",
        "        else:\n",
        "            p.requires_grad = False\n",
        "    return model, trainable, total\n",
        "\n",
        "\n",
        "def build_bert_lora(model_name, peft_cfg, num_labels=2):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=peft_cfg.lora_r,\n",
        "        lora_alpha=peft_cfg.lora_alpha,\n",
        "        lora_dropout=peft_cfg.lora_dropout,\n",
        "        target_modules=[\"query\", \"key\", \"value\"],\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"classifier\" in name:\n",
        "            p.requires_grad = True\n",
        "\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    return model, trainable, total\n",
        "\n",
        "\n",
        "def build_bert_prefix(model_name, peft_cfg, num_labels=2):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    prefix_cfg = PrefixTuningConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        num_virtual_tokens=peft_cfg.adapter_num_virtual_tokens\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, prefix_cfg)\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"classifier\" in name:\n",
        "            p.requires_grad = True\n",
        "\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    return model, trainable, total\n",
        "\n",
        "\n",
        "\n",
        "#RoBERT\n",
        "def build_roberta_fullft(model_name, num_labels=2):\n",
        "    return build_bert_fullft(model_name, num_labels)\n",
        "\n",
        "\n",
        "def build_roberta_bitfit(model_name, num_labels=2):\n",
        "    return build_bert_bitfit(model_name, num_labels)\n",
        "\n",
        "\n",
        "def build_roberta_lora(model_name, peft_cfg, num_labels=2):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=peft_cfg.lora_r,\n",
        "        lora_alpha=peft_cfg.lora_alpha,\n",
        "        lora_dropout=peft_cfg.lora_dropout,\n",
        "        target_modules=[\"query\", \"key\", \"value\"],  # Âêå BERT\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    return model, trainable, total\n",
        "\n",
        "def build_roberta_prefix(model_name, peft_cfg, num_labels=2):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    prefix_cfg = PrefixTuningConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        num_virtual_tokens=peft_cfg.adapter_num_virtual_tokens\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, prefix_cfg)\n",
        "\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    return model, trainable, total\n",
        "\n",
        "#T5\n",
        "from transformers import T5ForSequenceClassification\n",
        "\n",
        "def build_t5_fullft(model_name, num_labels=2):\n",
        "    model = T5ForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    return model, total, total\n",
        "\n",
        "\n",
        "def build_t5_bitfit(model_name, num_labels=2):\n",
        "    model = T5ForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = 0\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"bias\" in name or \"classification_head\" in name:\n",
        "            p.requires_grad = True\n",
        "            trainable += p.numel()\n",
        "        else:\n",
        "            p.requires_grad = False\n",
        "\n",
        "    return model, trainable, total\n",
        "\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "def build_t5_lora(model_name, peft_cfg, num_labels=2):\n",
        "    model = T5ForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=peft_cfg.lora_r,\n",
        "        lora_alpha=peft_cfg.lora_alpha,\n",
        "        lora_dropout=peft_cfg.lora_dropout,\n",
        "        target_modules=[\"q\", \"k\", \"v\", \"o\"],   # Ê≠£Á°ÆÁöÑ T5 Ê®°Âùó\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    return model, trainable, total\n",
        "\n",
        "from peft import PrefixTuningConfig\n",
        "\n",
        "def build_t5_prefix(model_name, peft_cfg, num_labels=2):\n",
        "    model = T5ForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    prefix_cfg = PrefixTuningConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        num_virtual_tokens=peft_cfg.adapter_num_virtual_tokens\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, prefix_cfg)\n",
        "\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    return model, trainable, total\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuI7xt0i3j2c"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KR_xqQ3y3knR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    eval_pred: Trainer ‰º†ÂÖ•ÁöÑÊ®°ÂûãËæìÂá∫ (logits, labels)\n",
        "    task_name: \"sst2\" or \"mrpc\"\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # HuggingFace Trainer ÁöÑ logits shape ‰∏∫ [batch, num_labels]\n",
        "    # Âèñ argmax ÂæóÂà∞ÂàÜÁ±ªÁªìÊûú\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Âä†ËΩΩ metrics\n",
        "    metric_acc = evaluate.load(\"accuracy\")\n",
        "    metric_f1 = evaluate.load(\"f1\")\n",
        "\n",
        "    acc = metric_acc.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "\n",
        "    # MRPC -> binary classification (0/1), same as SST2\n",
        "    # ÊâÄ‰ª• F1 ‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÂ§ÑÁêÜ\n",
        "    f1 = metric_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "\n",
        "def compute_t5_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Á®≥ÂÆöÁâà compute_t5_metricsÔºö\n",
        "    ÂÖºÂÆπ torch.Tensor / numpy.ndarray / (logits,) / Seq2Seq outputs\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # --- 1) T5 ÂèØËÉΩËæìÂá∫ (logits,) ---\n",
        "    if isinstance(logits, (tuple, list)):\n",
        "        logits = logits[0]\n",
        "\n",
        "    # --- 2) Êää logits ËΩ¨Êàê numpyÔºàÂÖºÂÆπ Tensor Âíå numpyÔºâ---\n",
        "    if hasattr(logits, \"detach\"):          # torch.Tensor\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "    else:                                   # numpy.ndarray\n",
        "        logits = np.asarray(logits)\n",
        "\n",
        "    # --- 3) labels ‰πüÁªü‰∏ÄÊàê numpy ---\n",
        "    if hasattr(labels, \"detach\"):           # tensor\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "    else:\n",
        "        labels = np.asarray(labels)\n",
        "\n",
        "    # --- 4) Â§öÁ±ª/‰∫åÂàÜÁ±ª argmax ---\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # --- 5) ËÆ°ÁÆóÊåáÊ†á ---\n",
        "    metric_acc = evaluate.load(\"accuracy\")\n",
        "    metric_f1 = evaluate.load(\"f1\")\n",
        "\n",
        "    acc = metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
        "    f1 = metric_f1.compute(predictions=preds, references=labels)[\"f1\"]\n",
        "\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVORXD-U3oKf"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU7-zUQI3mMt"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from pathlib import Path\n",
        "import math  # üëà NEW: for ceil\n",
        "\n",
        "def run_single_experiment(\n",
        "    config: ExperimentConfig,\n",
        "    raw_dataset,\n",
        "    model,\n",
        "    compute_metrics_fn,\n",
        "    trainable_params: int | None = None,\n",
        "    total_params: int | None = None,\n",
        "    results_dir: Path = Path(\"./results\"),\n",
        "    debug: bool = False,\n",
        "):\n",
        "\n",
        "    print(f\"\\n===== Running {config.model_name} / {config.task_name} / {config.peft_method} =====\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # Step 1 ‚Äî TokenizationÔºàÂê´ÁºìÂ≠òÔºâ\n",
        "    # ----------------------------------------\n",
        "    encoded_dataset, tokenizer = get_cached_tokenization(\n",
        "        config.model_name,\n",
        "        config.task_name,\n",
        "        raw_dataset,\n",
        "        max_length=config.training.max_length,\n",
        "    )\n",
        "\n",
        "    steps_per_epoch = math.ceil(\n",
        "        len(encoded_dataset[\"train\"]) / config.training.batch_size\n",
        "    )\n",
        "    half_epoch_steps = max(1, steps_per_epoch // 2)\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # Step 2 ‚Äî ÂèÇÊï∞ÁªüËÆ°ÔºàËã•Â§ñÈÉ®Ê≤°ÁÆóÔºåÂàôËá™Âä®ÁÆó‰∏ÄÊ¨°Ôºâ\n",
        "    # ----------------------------------------\n",
        "    if total_params is None:\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "    if trainable_params is None:\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # Step 3 ‚Äî Trainer ÈÖçÁΩÆ\n",
        "    # ----------------------------------------\n",
        "    output_dir = results_dir / f\"{config.task_name}_{config.peft_method}_{config.model_name.replace('/', '_')}\"\n",
        "\n",
        "    if debug:\n",
        "        print(\"debug mode on\")\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=str(output_dir),\n",
        "            per_device_train_batch_size=config.training.batch_size,\n",
        "            per_device_eval_batch_size=config.training.batch_size,\n",
        "            learning_rate=config.training.learning_rate,\n",
        "            num_train_epochs=1,\n",
        "            logging_steps=config.training.logging_steps,\n",
        "            evaluation_strategy=\"no\",   # ‚Üê HF arg name\n",
        "            save_strategy=\"no\",\n",
        "            report_to=[],\n",
        "            max_steps=3,\n",
        "        )\n",
        "    else:\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=str(output_dir),\n",
        "            per_device_train_batch_size=config.training.batch_size,\n",
        "            per_device_eval_batch_size=config.training.batch_size,\n",
        "            learning_rate=config.training.learning_rate,\n",
        "            num_train_epochs=config.training.num_epochs,\n",
        "\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=half_epoch_steps,\n",
        "\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=half_epoch_steps,\n",
        "\n",
        "            save_strategy=config.training.save_strategy,\n",
        "            report_to=[],\n",
        "        )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=encoded_dataset[\"train\"],\n",
        "        eval_dataset=encoded_dataset[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "        compute_metrics=compute_metrics_fn,\n",
        "    )\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # Step 4 ‚Äî ËÆ≠ÁªÉ + ËØÑ‰º∞\n",
        "    # ----------------------------------------\n",
        "    import time\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    start = time.time()\n",
        "    trainer.train()\n",
        "    eval_metrics = trainer.evaluate()\n",
        "    end = time.time()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_mem_mb = torch.cuda.max_memory_allocated() / 1024**2\n",
        "    else:\n",
        "        gpu_mem_mb = 0.0\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # Extract Epoch-wise History\n",
        "    # --------------s--------------------------\n",
        "    train_history = []\n",
        "    eval_history = []\n",
        "    best_f1 = -1.0\n",
        "    best_acc = -1.0\n",
        "    best_f1_epoch = None\n",
        "    best_acc_epoch = None\n",
        "\n",
        "    for log in trainer.state.log_history:\n",
        "        if \"loss\" in log:\n",
        "            train_history.append({\n",
        "                \"epoch\": log.get(\"epoch\"),\n",
        "                \"loss\": log[\"loss\"]\n",
        "            })\n",
        "        elif \"eval_loss\" in log:\n",
        "            eval_history.append({\n",
        "                \"epoch\": log.get(\"epoch\"),\n",
        "                \"eval_loss\": log[\"eval_loss\"],\n",
        "                \"eval_accuracy\": log.get(\"eval_accuracy\"),\n",
        "                \"eval_f1\": log.get(\"eval_f1\")\n",
        "            })\n",
        "\n",
        "        if \"eval_f1\" in log:\n",
        "            if log[\"eval_f1\"] > best_f1:\n",
        "                best_f1 = float(log[\"eval_f1\"])\n",
        "                best_f1_epoch = log.get(\"epoch\")\n",
        "\n",
        "        if \"eval_accuracy\" in log:\n",
        "            if log[\"eval_accuracy\"] > best_acc:\n",
        "                best_acc = float(log[\"eval_accuracy\"])\n",
        "                best_acc_epoch = log.get(\"epoch\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # Step 5 ‚Äî Ê±áÊÄªÁªìÊûú\n",
        "    # ----------------------------------------\n",
        "    result = {\n",
        "        \"task\": config.task_name,\n",
        "        \"model_name\": config.model_name,\n",
        "        \"peft_method\": config.peft_method,\n",
        "        \"val_accuracy\": float(eval_metrics.get(\"eval_accuracy\", 0)),\n",
        "        \"val_f1\": float(eval_metrics.get(\"eval_f1\", 0)),\n",
        "        \"train_time_sec\": end - start,\n",
        "        \"gpu_mem_mb\": gpu_mem_mb,  # üëà now per-experiment\n",
        "        \"trainable_params\": int(trainable_params),\n",
        "        \"total_params\": int(total_params),\n",
        "\n",
        "        \"best_val_accuracy\": best_acc,\n",
        "        \"best_val_accuracy_epoch\": best_acc_epoch,\n",
        "        \"best_val_f1\": best_f1,\n",
        "        \"best_val_f1_epoch\": best_f1_epoch,\n",
        "        \"history\": {\n",
        "            \"train\": train_history,\n",
        "            \"eval\": eval_history\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # Step 6 ‚Äî ‰øùÂ≠òÁªìÊûú\n",
        "    # ----------------------------------------\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_dir / \"result.json\", \"w\") as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "\n",
        "    print(\" Finished. Metrics:\", result)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWQA-dOC6pBW"
      },
      "source": [
        "# Run Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SrOWijQ-6p5V"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "def get_builder(model_name: str, peft_method: str):\n",
        "\n",
        "    name = model_name.lower()\n",
        "\n",
        "    # T5\n",
        "    if \"t5\" in name:\n",
        "        if peft_method == \"full_ft\":\n",
        "            return build_t5_fullft\n",
        "        elif peft_method == \"bitfit\":\n",
        "            return build_t5_bitfit\n",
        "        elif peft_method == \"lora\":\n",
        "            return build_t5_lora\n",
        "        elif peft_method == \"adapter\":\n",
        "            return build_t5_adapter\n",
        "        else:\n",
        "            raise ValueError(f\"T5 does not support PEFT method: {peft_method}\")\n",
        "\n",
        "    # BERT\n",
        "        # BERT\n",
        "    if \"bert\" in name:\n",
        "        if peft_method == \"full_ft\":\n",
        "            return build_bert_fullft\n",
        "        elif peft_method == \"bitfit\":\n",
        "            return build_bert_bitfit\n",
        "        elif peft_method == \"lora\":\n",
        "            return build_bert_lora\n",
        "        elif peft_method == \"prefix\":\n",
        "            return build_bert_prefix\n",
        "        elif peft_method == \"adapter\":\n",
        "            return build_bert_adapter\n",
        "        else:\n",
        "            raise ValueError(f\"BERT does not support PEFT method: {peft_method}\")\n",
        "\n",
        "    # RoBERTa\n",
        "    if \"roberta\" in name:\n",
        "        if peft_method == \"full_ft\":\n",
        "            return build_roberta_fullft\n",
        "        elif peft_method == \"bitfit\":\n",
        "            return build_roberta_bitfit\n",
        "        elif peft_method == \"lora\":\n",
        "            return build_roberta_lora\n",
        "        else:\n",
        "            raise ValueError(f\"RoBERTa does not support PEFT method: {peft_method}\")\n",
        "\n",
        "\n",
        "    raise ValueError(f\"Unknown model family for: {model_name}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "vOzTHEfWCDij"
      },
      "outputs": [],
      "source": [
        "\n",
        "metrics_fn_dict = {\n",
        "    \"t5-small\": compute_t5_metrics,\n",
        "    \"t5-base\": compute_t5_metrics,\n",
        "    \"bert-base-uncased\": compute_metrics,\n",
        "    \"roberta-base\": compute_metrics\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AvJkyi566rsM",
        "outputId": "12d268ec-3e73-4df4-9893-b10064b47e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### Running: bert-base-uncased | sst2 | full_ft ###\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-883097910.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running bert-base-uncased / sst2 / full_ft =====\n",
            "‚úì Loading cached tokenization from: cache/tokenized/bert-base-uncased_sst2_128.pkl\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [189/189 00:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.517700</td>\n",
              "      <td>0.407116</td>\n",
              "      <td>0.843333</td>\n",
              "      <td>0.858859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.321400</td>\n",
              "      <td>0.351952</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.866469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.249200</td>\n",
              "      <td>0.307034</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>0.305653</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.876543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.188500</td>\n",
              "      <td>0.290471</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.877743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.172000</td>\n",
              "      <td>0.291119</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.878505</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Finished. Metrics: {'task': 'sst2', 'model_name': 'bert-base-uncased', 'peft_method': 'full_ft', 'val_accuracy': 0.87, 'val_f1': 0.8785046728971962, 'train_time_sec': 60.68808650970459, 'gpu_mem_mb': 6129.65234375, 'trainable_params': 109483778, 'total_params': 109483778, 'history': {'train': [{'epoch': 0.49206349206349204, 'loss': 0.5177}, {'epoch': 0.9841269841269841, 'loss': 0.3214}, {'epoch': 1.4761904761904763, 'loss': 0.2492}, {'epoch': 1.9682539682539684, 'loss': 0.228}, {'epoch': 2.4603174603174605, 'loss': 0.1885}, {'epoch': 2.9523809523809526, 'loss': 0.172}], 'eval': [{'epoch': 0.49206349206349204, 'eval_loss': 0.4071155786514282, 'eval_accuracy': 0.8433333333333334, 'eval_f1': 0.8588588588588588}, {'epoch': 0.9841269841269841, 'eval_loss': 0.35195192694664, 'eval_accuracy': 0.85, 'eval_f1': 0.8664688427299704}, {'epoch': 1.4761904761904763, 'eval_loss': 0.3070344626903534, 'eval_accuracy': 0.8666666666666667, 'eval_f1': 0.875}, {'epoch': 1.9682539682539684, 'eval_loss': 0.30565255880355835, 'eval_accuracy': 0.8666666666666667, 'eval_f1': 0.8765432098765432}, {'epoch': 2.4603174603174605, 'eval_loss': 0.29047054052352905, 'eval_accuracy': 0.87, 'eval_f1': 0.877742946708464}, {'epoch': 2.9523809523809526, 'eval_loss': 0.29111912846565247, 'eval_accuracy': 0.87, 'eval_f1': 0.8785046728971962}, {'epoch': 3.0, 'eval_loss': 0.29116326570510864, 'eval_accuracy': 0.87, 'eval_f1': 0.8785046728971962}]}}\n",
            "\n",
            "### Running: bert-base-uncased | sst2 | bitfit ###\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-883097910.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running bert-base-uncased / sst2 / bitfit =====\n",
            "‚úì Using cached tokenization (memory) for bert-base-uncased/sst2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [189/189 00:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.665100</td>\n",
              "      <td>0.530340</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.423300</td>\n",
              "      <td>0.381130</td>\n",
              "      <td>0.816667</td>\n",
              "      <td>0.832827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.359700</td>\n",
              "      <td>0.341395</td>\n",
              "      <td>0.846667</td>\n",
              "      <td>0.848684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.319300</td>\n",
              "      <td>0.353931</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.872727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.309500</td>\n",
              "      <td>0.329262</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.869565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.304500</td>\n",
              "      <td>0.324276</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.866242</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Finished. Metrics: {'task': 'sst2', 'model_name': 'bert-base-uncased', 'peft_method': 'bitfit', 'val_accuracy': 0.86, 'val_f1': 0.8662420382165605, 'train_time_sec': 49.029844999313354, 'gpu_mem_mb': 3651.7490234375, 'trainable_params': 104450, 'total_params': 109483778, 'history': {'train': [{'epoch': 0.49206349206349204, 'loss': 0.6651}, {'epoch': 0.9841269841269841, 'loss': 0.4233}, {'epoch': 1.4761904761904763, 'loss': 0.3597}, {'epoch': 1.9682539682539684, 'loss': 0.3193}, {'epoch': 2.4603174603174605, 'loss': 0.3095}, {'epoch': 2.9523809523809526, 'loss': 0.3045}], 'eval': [{'epoch': 0.49206349206349204, 'eval_loss': 0.5303401947021484, 'eval_accuracy': 0.7466666666666667, 'eval_f1': 0.7777777777777778}, {'epoch': 0.9841269841269841, 'eval_loss': 0.3811303973197937, 'eval_accuracy': 0.8166666666666667, 'eval_f1': 0.8328267477203647}, {'epoch': 1.4761904761904763, 'eval_loss': 0.3413947522640228, 'eval_accuracy': 0.8466666666666667, 'eval_f1': 0.8486842105263158}, {'epoch': 1.9682539682539684, 'eval_loss': 0.3539314866065979, 'eval_accuracy': 0.86, 'eval_f1': 0.8727272727272727}, {'epoch': 2.4603174603174605, 'eval_loss': 0.3292623460292816, 'eval_accuracy': 0.86, 'eval_f1': 0.8695652173913043}, {'epoch': 2.9523809523809526, 'eval_loss': 0.3242759108543396, 'eval_accuracy': 0.86, 'eval_f1': 0.8662420382165605}, {'epoch': 3.0, 'eval_loss': 0.3243158757686615, 'eval_accuracy': 0.86, 'eval_f1': 0.8662420382165605}]}}\n",
            "\n",
            "### Running: bert-base-uncased | sst2 | lora ###\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-883097910.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running bert-base-uncased / sst2 / lora =====\n",
            "‚úì Using cached tokenization (memory) for bert-base-uncased/sst2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [189/189 00:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.565900</td>\n",
              "      <td>0.401224</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.843137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.330800</td>\n",
              "      <td>0.367908</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.848485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.275900</td>\n",
              "      <td>0.313783</td>\n",
              "      <td>0.883333</td>\n",
              "      <td>0.885993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.259600</td>\n",
              "      <td>0.329588</td>\n",
              "      <td>0.883333</td>\n",
              "      <td>0.886731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.226000</td>\n",
              "      <td>0.307786</td>\n",
              "      <td>0.873333</td>\n",
              "      <td>0.879747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.218200</td>\n",
              "      <td>0.317518</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.886076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Finished. Metrics: {'task': 'sst2', 'model_name': 'bert-base-uncased', 'peft_method': 'lora', 'val_accuracy': 0.88, 'val_f1': 0.8860759493670886, 'train_time_sec': 50.958537340164185, 'gpu_mem_mb': 4668.67822265625, 'trainable_params': 445444, 'total_params': 109927684, 'history': {'train': [{'epoch': 0.49206349206349204, 'loss': 0.5659}, {'epoch': 0.9841269841269841, 'loss': 0.3308}, {'epoch': 1.4761904761904763, 'loss': 0.2759}, {'epoch': 1.9682539682539684, 'loss': 0.2596}, {'epoch': 2.4603174603174605, 'loss': 0.226}, {'epoch': 2.9523809523809526, 'loss': 0.2182}], 'eval': [{'epoch': 0.49206349206349204, 'eval_loss': 0.40122437477111816, 'eval_accuracy': 0.84, 'eval_f1': 0.8431372549019608}, {'epoch': 0.9841269841269841, 'eval_loss': 0.3679078221321106, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.8484848484848485}, {'epoch': 1.4761904761904763, 'eval_loss': 0.31378310918807983, 'eval_accuracy': 0.8833333333333333, 'eval_f1': 0.8859934853420195}, {'epoch': 1.9682539682539684, 'eval_loss': 0.32958775758743286, 'eval_accuracy': 0.8833333333333333, 'eval_f1': 0.8867313915857605}, {'epoch': 2.4603174603174605, 'eval_loss': 0.30778563022613525, 'eval_accuracy': 0.8733333333333333, 'eval_f1': 0.879746835443038}, {'epoch': 2.9523809523809526, 'eval_loss': 0.3175179660320282, 'eval_accuracy': 0.88, 'eval_f1': 0.8860759493670886}, {'epoch': 3.0, 'eval_loss': 0.31767594814300537, 'eval_accuracy': 0.88, 'eval_f1': 0.8860759493670886}]}}\n",
            "\n",
            "### Running: bert-base-uncased | sst2 | prefix ###\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-883097910.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running bert-base-uncased / sst2 / prefix =====\n",
            "‚úì Using cached tokenization (memory) for bert-base-uncased/sst2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [189/189 00:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.683700</td>\n",
              "      <td>0.692653</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.682819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.659400</td>\n",
              "      <td>0.638031</td>\n",
              "      <td>0.576667</td>\n",
              "      <td>0.703963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.579600</td>\n",
              "      <td>0.558169</td>\n",
              "      <td>0.656667</td>\n",
              "      <td>0.739241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.508800</td>\n",
              "      <td>0.498760</td>\n",
              "      <td>0.716667</td>\n",
              "      <td>0.768392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.464200</td>\n",
              "      <td>0.476969</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.788732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.454200</td>\n",
              "      <td>0.456956</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>0.811940</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Finished. Metrics: {'task': 'sst2', 'model_name': 'bert-base-uncased', 'peft_method': 'prefix', 'val_accuracy': 0.79, 'val_f1': 0.8119402985074626, 'train_time_sec': 48.944904088974, 'gpu_mem_mb': 3820.6875, 'trainable_params': 371716, 'total_params': 109853956, 'history': {'train': [{'epoch': 0.49206349206349204, 'loss': 0.6837}, {'epoch': 0.9841269841269841, 'loss': 0.6594}, {'epoch': 1.4761904761904763, 'loss': 0.5796}, {'epoch': 1.9682539682539684, 'loss': 0.5088}, {'epoch': 2.4603174603174605, 'loss': 0.4642}, {'epoch': 2.9523809523809526, 'loss': 0.4542}], 'eval': [{'epoch': 0.49206349206349204, 'eval_loss': 0.6926532983779907, 'eval_accuracy': 0.52, 'eval_f1': 0.6828193832599119}, {'epoch': 0.9841269841269841, 'eval_loss': 0.6380308270454407, 'eval_accuracy': 0.5766666666666667, 'eval_f1': 0.703962703962704}, {'epoch': 1.4761904761904763, 'eval_loss': 0.5581691861152649, 'eval_accuracy': 0.6566666666666666, 'eval_f1': 0.739240506329114}, {'epoch': 1.9682539682539684, 'eval_loss': 0.4987603724002838, 'eval_accuracy': 0.7166666666666667, 'eval_f1': 0.7683923705722071}, {'epoch': 2.4603174603174605, 'eval_loss': 0.4769692122936249, 'eval_accuracy': 0.75, 'eval_f1': 0.7887323943661971}, {'epoch': 2.9523809523809526, 'eval_loss': 0.4569559693336487, 'eval_accuracy': 0.79, 'eval_f1': 0.8119402985074626}, {'epoch': 3.0, 'eval_loss': 0.45706263184547424, 'eval_accuracy': 0.79, 'eval_f1': 0.8119402985074626}]}}\n",
            "\n",
            "### Running: bert-base-uncased | mrpc | full_ft ###\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-883097910.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running bert-base-uncased / mrpc / full_ft =====\n",
            "‚úì Loading cached tokenization from: cache/tokenized/bert-base-uncased_mrpc_128.pkl\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 00:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.717600</td>\n",
              "      <td>0.688925</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.809524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.683700</td>\n",
              "      <td>0.658641</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.629600</td>\n",
              "      <td>0.632551</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.648800</td>\n",
              "      <td>0.620644</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.629600</td>\n",
              "      <td>0.614354</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.604200</td>\n",
              "      <td>0.610448</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.678600</td>\n",
              "      <td>0.608899</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Finished. Metrics: {'task': 'mrpc', 'model_name': 'bert-base-uncased', 'peft_method': 'full_ft', 'val_accuracy': 0.685, 'val_f1': 0.8130563798219584, 'train_time_sec': 28.978463888168335, 'gpu_mem_mb': 8244.8427734375, 'trainable_params': 109483778, 'total_params': 109483778, 'history': {'train': [{'epoch': 0.42857142857142855, 'loss': 0.7176}, {'epoch': 0.8571428571428571, 'loss': 0.6837}, {'epoch': 1.2857142857142856, 'loss': 0.6296}, {'epoch': 1.7142857142857144, 'loss': 0.6488}, {'epoch': 2.142857142857143, 'loss': 0.6296}, {'epoch': 2.571428571428571, 'loss': 0.6042}, {'epoch': 3.0, 'loss': 0.6786}], 'eval': [{'epoch': 0.42857142857142855, 'eval_loss': 0.6889253854751587, 'eval_accuracy': 0.68, 'eval_f1': 0.8095238095238095}, {'epoch': 0.8571428571428571, 'eval_loss': 0.6586406230926514, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 1.2857142857142856, 'eval_loss': 0.632550835609436, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 1.7142857142857144, 'eval_loss': 0.6206437945365906, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 2.142857142857143, 'eval_loss': 0.6143536567687988, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 2.571428571428571, 'eval_loss': 0.6104475259780884, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 3.0, 'eval_loss': 0.6088988780975342, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 3.0, 'eval_loss': 0.6088988780975342, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}]}}\n",
            "\n",
            "### Running: bert-base-uncased | mrpc | bitfit ###\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-883097910.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running bert-base-uncased / mrpc / bitfit =====\n",
            "‚úì Using cached tokenization (memory) for bert-base-uncased/mrpc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 00:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.793900</td>\n",
              "      <td>0.666437</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.661300</td>\n",
              "      <td>0.648188</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.575800</td>\n",
              "      <td>0.607450</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.649400</td>\n",
              "      <td>0.652978</td>\n",
              "      <td>0.695000</td>\n",
              "      <td>0.791809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.651100</td>\n",
              "      <td>0.618405</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.827160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.616100</td>\n",
              "      <td>0.593852</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.596413</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Finished. Metrics: {'task': 'mrpc', 'model_name': 'bert-base-uncased', 'peft_method': 'bitfit', 'val_accuracy': 0.685, 'val_f1': 0.8130563798219584, 'train_time_sec': 26.72392702102661, 'gpu_mem_mb': 5054.01123046875, 'trainable_params': 104450, 'total_params': 109483778, 'history': {'train': [{'epoch': 0.42857142857142855, 'loss': 0.7939}, {'epoch': 0.8571428571428571, 'loss': 0.6613}, {'epoch': 1.2857142857142856, 'loss': 0.5758}, {'epoch': 1.7142857142857144, 'loss': 0.6494}, {'epoch': 2.142857142857143, 'loss': 0.6511}, {'epoch': 2.571428571428571, 'loss': 0.6161}, {'epoch': 3.0, 'loss': 0.65}], 'eval': [{'epoch': 0.42857142857142855, 'eval_loss': 0.6664366126060486, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 0.8571428571428571, 'eval_loss': 0.6481884717941284, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 1.2857142857142856, 'eval_loss': 0.6074501872062683, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 1.7142857142857144, 'eval_loss': 0.6529775857925415, 'eval_accuracy': 0.695, 'eval_f1': 0.7918088737201365}, {'epoch': 2.142857142857143, 'eval_loss': 0.6184054613113403, 'eval_accuracy': 0.72, 'eval_f1': 0.8271604938271605}, {'epoch': 2.571428571428571, 'eval_loss': 0.5938519835472107, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 3.0, 'eval_loss': 0.5964128971099854, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 3.0, 'eval_loss': 0.5964128971099854, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}]}}\n",
            "\n",
            "### Running: bert-base-uncased | mrpc | lora ###\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-883097910.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running bert-base-uncased / mrpc / lora =====\n",
            "‚úì Using cached tokenization (memory) for bert-base-uncased/mrpc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 00:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.794500</td>\n",
              "      <td>0.680852</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.669800</td>\n",
              "      <td>0.684060</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.588800</td>\n",
              "      <td>0.618110</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.666400</td>\n",
              "      <td>0.696060</td>\n",
              "      <td>0.485000</td>\n",
              "      <td>0.477157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.713000</td>\n",
              "      <td>0.693948</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.679800</td>\n",
              "      <td>0.636772</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.831804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.662100</td>\n",
              "      <td>0.615204</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.815476</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Finished. Metrics: {'task': 'mrpc', 'model_name': 'bert-base-uncased', 'peft_method': 'lora', 'val_accuracy': 0.69, 'val_f1': 0.8154761904761905, 'train_time_sec': 27.116196632385254, 'gpu_mem_mb': 6505.8359375, 'trainable_params': 445444, 'total_params': 109927684, 'history': {'train': [{'epoch': 0.42857142857142855, 'loss': 0.7945}, {'epoch': 0.8571428571428571, 'loss': 0.6698}, {'epoch': 1.2857142857142856, 'loss': 0.5888}, {'epoch': 1.7142857142857144, 'loss': 0.6664}, {'epoch': 2.142857142857143, 'loss': 0.713}, {'epoch': 2.571428571428571, 'loss': 0.6798}, {'epoch': 3.0, 'loss': 0.6621}], 'eval': [{'epoch': 0.42857142857142855, 'eval_loss': 0.6808517575263977, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 0.8571428571428571, 'eval_loss': 0.684060275554657, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 1.2857142857142856, 'eval_loss': 0.6181102991104126, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 1.7142857142857144, 'eval_loss': 0.6960600018501282, 'eval_accuracy': 0.485, 'eval_f1': 0.47715736040609136}, {'epoch': 2.142857142857143, 'eval_loss': 0.6939476132392883, 'eval_accuracy': 0.52, 'eval_f1': 0.52}, {'epoch': 2.571428571428571, 'eval_loss': 0.6367719173431396, 'eval_accuracy': 0.725, 'eval_f1': 0.8318042813455657}, {'epoch': 3.0, 'eval_loss': 0.615203857421875, 'eval_accuracy': 0.69, 'eval_f1': 0.8154761904761905}, {'epoch': 3.0, 'eval_loss': 0.615203857421875, 'eval_accuracy': 0.69, 'eval_f1': 0.8154761904761905}]}}\n",
            "\n",
            "### Running: bert-base-uncased | mrpc | prefix ###\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-883097910.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running bert-base-uncased / mrpc / prefix =====\n",
            "‚úì Using cached tokenization (memory) for bert-base-uncased/mrpc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 00:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.782400</td>\n",
              "      <td>0.683628</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.815476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.681800</td>\n",
              "      <td>0.695499</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.634747</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.661902</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.670800</td>\n",
              "      <td>0.661753</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.660900</td>\n",
              "      <td>0.628419</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.659900</td>\n",
              "      <td>0.621011</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.813056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Finished. Metrics: {'task': 'mrpc', 'model_name': 'bert-base-uncased', 'peft_method': 'prefix', 'val_accuracy': 0.685, 'val_f1': 0.8130563798219584, 'train_time_sec': 26.86567759513855, 'gpu_mem_mb': 5205.98486328125, 'trainable_params': 371716, 'total_params': 109853956, 'history': {'train': [{'epoch': 0.42857142857142855, 'loss': 0.7824}, {'epoch': 0.8571428571428571, 'loss': 0.6818}, {'epoch': 1.2857142857142856, 'loss': 0.625}, {'epoch': 1.7142857142857144, 'loss': 0.671}, {'epoch': 2.142857142857143, 'loss': 0.6708}, {'epoch': 2.571428571428571, 'loss': 0.6609}, {'epoch': 3.0, 'loss': 0.6599}], 'eval': [{'epoch': 0.42857142857142855, 'eval_loss': 0.683628261089325, 'eval_accuracy': 0.69, 'eval_f1': 0.8154761904761905}, {'epoch': 0.8571428571428571, 'eval_loss': 0.6954988241195679, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 1.2857142857142856, 'eval_loss': 0.6347468495368958, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 1.7142857142857144, 'eval_loss': 0.6619020700454712, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 2.142857142857143, 'eval_loss': 0.6617526412010193, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 2.571428571428571, 'eval_loss': 0.628418505191803, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 3.0, 'eval_loss': 0.6210105419158936, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}, {'epoch': 3.0, 'eval_loss': 0.6210105419158936, 'eval_accuracy': 0.685, 'eval_f1': 0.8130563798219584}]}}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'task': 'sst2',\n",
              "  'model_name': 'bert-base-uncased',\n",
              "  'peft_method': 'full_ft',\n",
              "  'val_accuracy': 0.87,\n",
              "  'val_f1': 0.8785046728971962,\n",
              "  'train_time_sec': 60.68808650970459,\n",
              "  'gpu_mem_mb': 6129.65234375,\n",
              "  'trainable_params': 109483778,\n",
              "  'total_params': 109483778,\n",
              "  'history': {'train': [{'epoch': 0.49206349206349204, 'loss': 0.5177},\n",
              "    {'epoch': 0.9841269841269841, 'loss': 0.3214},\n",
              "    {'epoch': 1.4761904761904763, 'loss': 0.2492},\n",
              "    {'epoch': 1.9682539682539684, 'loss': 0.228},\n",
              "    {'epoch': 2.4603174603174605, 'loss': 0.1885},\n",
              "    {'epoch': 2.9523809523809526, 'loss': 0.172}],\n",
              "   'eval': [{'epoch': 0.49206349206349204,\n",
              "     'eval_loss': 0.4071155786514282,\n",
              "     'eval_accuracy': 0.8433333333333334,\n",
              "     'eval_f1': 0.8588588588588588},\n",
              "    {'epoch': 0.9841269841269841,\n",
              "     'eval_loss': 0.35195192694664,\n",
              "     'eval_accuracy': 0.85,\n",
              "     'eval_f1': 0.8664688427299704},\n",
              "    {'epoch': 1.4761904761904763,\n",
              "     'eval_loss': 0.3070344626903534,\n",
              "     'eval_accuracy': 0.8666666666666667,\n",
              "     'eval_f1': 0.875},\n",
              "    {'epoch': 1.9682539682539684,\n",
              "     'eval_loss': 0.30565255880355835,\n",
              "     'eval_accuracy': 0.8666666666666667,\n",
              "     'eval_f1': 0.8765432098765432},\n",
              "    {'epoch': 2.4603174603174605,\n",
              "     'eval_loss': 0.29047054052352905,\n",
              "     'eval_accuracy': 0.87,\n",
              "     'eval_f1': 0.877742946708464},\n",
              "    {'epoch': 2.9523809523809526,\n",
              "     'eval_loss': 0.29111912846565247,\n",
              "     'eval_accuracy': 0.87,\n",
              "     'eval_f1': 0.8785046728971962},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.29116326570510864,\n",
              "     'eval_accuracy': 0.87,\n",
              "     'eval_f1': 0.8785046728971962}]}},\n",
              " {'task': 'sst2',\n",
              "  'model_name': 'bert-base-uncased',\n",
              "  'peft_method': 'bitfit',\n",
              "  'val_accuracy': 0.86,\n",
              "  'val_f1': 0.8662420382165605,\n",
              "  'train_time_sec': 49.029844999313354,\n",
              "  'gpu_mem_mb': 3651.7490234375,\n",
              "  'trainable_params': 104450,\n",
              "  'total_params': 109483778,\n",
              "  'history': {'train': [{'epoch': 0.49206349206349204, 'loss': 0.6651},\n",
              "    {'epoch': 0.9841269841269841, 'loss': 0.4233},\n",
              "    {'epoch': 1.4761904761904763, 'loss': 0.3597},\n",
              "    {'epoch': 1.9682539682539684, 'loss': 0.3193},\n",
              "    {'epoch': 2.4603174603174605, 'loss': 0.3095},\n",
              "    {'epoch': 2.9523809523809526, 'loss': 0.3045}],\n",
              "   'eval': [{'epoch': 0.49206349206349204,\n",
              "     'eval_loss': 0.5303401947021484,\n",
              "     'eval_accuracy': 0.7466666666666667,\n",
              "     'eval_f1': 0.7777777777777778},\n",
              "    {'epoch': 0.9841269841269841,\n",
              "     'eval_loss': 0.3811303973197937,\n",
              "     'eval_accuracy': 0.8166666666666667,\n",
              "     'eval_f1': 0.8328267477203647},\n",
              "    {'epoch': 1.4761904761904763,\n",
              "     'eval_loss': 0.3413947522640228,\n",
              "     'eval_accuracy': 0.8466666666666667,\n",
              "     'eval_f1': 0.8486842105263158},\n",
              "    {'epoch': 1.9682539682539684,\n",
              "     'eval_loss': 0.3539314866065979,\n",
              "     'eval_accuracy': 0.86,\n",
              "     'eval_f1': 0.8727272727272727},\n",
              "    {'epoch': 2.4603174603174605,\n",
              "     'eval_loss': 0.3292623460292816,\n",
              "     'eval_accuracy': 0.86,\n",
              "     'eval_f1': 0.8695652173913043},\n",
              "    {'epoch': 2.9523809523809526,\n",
              "     'eval_loss': 0.3242759108543396,\n",
              "     'eval_accuracy': 0.86,\n",
              "     'eval_f1': 0.8662420382165605},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.3243158757686615,\n",
              "     'eval_accuracy': 0.86,\n",
              "     'eval_f1': 0.8662420382165605}]}},\n",
              " {'task': 'sst2',\n",
              "  'model_name': 'bert-base-uncased',\n",
              "  'peft_method': 'lora',\n",
              "  'val_accuracy': 0.88,\n",
              "  'val_f1': 0.8860759493670886,\n",
              "  'train_time_sec': 50.958537340164185,\n",
              "  'gpu_mem_mb': 4668.67822265625,\n",
              "  'trainable_params': 445444,\n",
              "  'total_params': 109927684,\n",
              "  'history': {'train': [{'epoch': 0.49206349206349204, 'loss': 0.5659},\n",
              "    {'epoch': 0.9841269841269841, 'loss': 0.3308},\n",
              "    {'epoch': 1.4761904761904763, 'loss': 0.2759},\n",
              "    {'epoch': 1.9682539682539684, 'loss': 0.2596},\n",
              "    {'epoch': 2.4603174603174605, 'loss': 0.226},\n",
              "    {'epoch': 2.9523809523809526, 'loss': 0.2182}],\n",
              "   'eval': [{'epoch': 0.49206349206349204,\n",
              "     'eval_loss': 0.40122437477111816,\n",
              "     'eval_accuracy': 0.84,\n",
              "     'eval_f1': 0.8431372549019608},\n",
              "    {'epoch': 0.9841269841269841,\n",
              "     'eval_loss': 0.3679078221321106,\n",
              "     'eval_accuracy': 0.8333333333333334,\n",
              "     'eval_f1': 0.8484848484848485},\n",
              "    {'epoch': 1.4761904761904763,\n",
              "     'eval_loss': 0.31378310918807983,\n",
              "     'eval_accuracy': 0.8833333333333333,\n",
              "     'eval_f1': 0.8859934853420195},\n",
              "    {'epoch': 1.9682539682539684,\n",
              "     'eval_loss': 0.32958775758743286,\n",
              "     'eval_accuracy': 0.8833333333333333,\n",
              "     'eval_f1': 0.8867313915857605},\n",
              "    {'epoch': 2.4603174603174605,\n",
              "     'eval_loss': 0.30778563022613525,\n",
              "     'eval_accuracy': 0.8733333333333333,\n",
              "     'eval_f1': 0.879746835443038},\n",
              "    {'epoch': 2.9523809523809526,\n",
              "     'eval_loss': 0.3175179660320282,\n",
              "     'eval_accuracy': 0.88,\n",
              "     'eval_f1': 0.8860759493670886},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.31767594814300537,\n",
              "     'eval_accuracy': 0.88,\n",
              "     'eval_f1': 0.8860759493670886}]}},\n",
              " {'task': 'sst2',\n",
              "  'model_name': 'bert-base-uncased',\n",
              "  'peft_method': 'prefix',\n",
              "  'val_accuracy': 0.79,\n",
              "  'val_f1': 0.8119402985074626,\n",
              "  'train_time_sec': 48.944904088974,\n",
              "  'gpu_mem_mb': 3820.6875,\n",
              "  'trainable_params': 371716,\n",
              "  'total_params': 109853956,\n",
              "  'history': {'train': [{'epoch': 0.49206349206349204, 'loss': 0.6837},\n",
              "    {'epoch': 0.9841269841269841, 'loss': 0.6594},\n",
              "    {'epoch': 1.4761904761904763, 'loss': 0.5796},\n",
              "    {'epoch': 1.9682539682539684, 'loss': 0.5088},\n",
              "    {'epoch': 2.4603174603174605, 'loss': 0.4642},\n",
              "    {'epoch': 2.9523809523809526, 'loss': 0.4542}],\n",
              "   'eval': [{'epoch': 0.49206349206349204,\n",
              "     'eval_loss': 0.6926532983779907,\n",
              "     'eval_accuracy': 0.52,\n",
              "     'eval_f1': 0.6828193832599119},\n",
              "    {'epoch': 0.9841269841269841,\n",
              "     'eval_loss': 0.6380308270454407,\n",
              "     'eval_accuracy': 0.5766666666666667,\n",
              "     'eval_f1': 0.703962703962704},\n",
              "    {'epoch': 1.4761904761904763,\n",
              "     'eval_loss': 0.5581691861152649,\n",
              "     'eval_accuracy': 0.6566666666666666,\n",
              "     'eval_f1': 0.739240506329114},\n",
              "    {'epoch': 1.9682539682539684,\n",
              "     'eval_loss': 0.4987603724002838,\n",
              "     'eval_accuracy': 0.7166666666666667,\n",
              "     'eval_f1': 0.7683923705722071},\n",
              "    {'epoch': 2.4603174603174605,\n",
              "     'eval_loss': 0.4769692122936249,\n",
              "     'eval_accuracy': 0.75,\n",
              "     'eval_f1': 0.7887323943661971},\n",
              "    {'epoch': 2.9523809523809526,\n",
              "     'eval_loss': 0.4569559693336487,\n",
              "     'eval_accuracy': 0.79,\n",
              "     'eval_f1': 0.8119402985074626},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.45706263184547424,\n",
              "     'eval_accuracy': 0.79,\n",
              "     'eval_f1': 0.8119402985074626}]}},\n",
              " {'task': 'mrpc',\n",
              "  'model_name': 'bert-base-uncased',\n",
              "  'peft_method': 'full_ft',\n",
              "  'val_accuracy': 0.685,\n",
              "  'val_f1': 0.8130563798219584,\n",
              "  'train_time_sec': 28.978463888168335,\n",
              "  'gpu_mem_mb': 8244.8427734375,\n",
              "  'trainable_params': 109483778,\n",
              "  'total_params': 109483778,\n",
              "  'history': {'train': [{'epoch': 0.42857142857142855, 'loss': 0.7176},\n",
              "    {'epoch': 0.8571428571428571, 'loss': 0.6837},\n",
              "    {'epoch': 1.2857142857142856, 'loss': 0.6296},\n",
              "    {'epoch': 1.7142857142857144, 'loss': 0.6488},\n",
              "    {'epoch': 2.142857142857143, 'loss': 0.6296},\n",
              "    {'epoch': 2.571428571428571, 'loss': 0.6042},\n",
              "    {'epoch': 3.0, 'loss': 0.6786}],\n",
              "   'eval': [{'epoch': 0.42857142857142855,\n",
              "     'eval_loss': 0.6889253854751587,\n",
              "     'eval_accuracy': 0.68,\n",
              "     'eval_f1': 0.8095238095238095},\n",
              "    {'epoch': 0.8571428571428571,\n",
              "     'eval_loss': 0.6586406230926514,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 1.2857142857142856,\n",
              "     'eval_loss': 0.632550835609436,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 1.7142857142857144,\n",
              "     'eval_loss': 0.6206437945365906,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 2.142857142857143,\n",
              "     'eval_loss': 0.6143536567687988,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 2.571428571428571,\n",
              "     'eval_loss': 0.6104475259780884,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.6088988780975342,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.6088988780975342,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584}]}},\n",
              " {'task': 'mrpc',\n",
              "  'model_name': 'bert-base-uncased',\n",
              "  'peft_method': 'bitfit',\n",
              "  'val_accuracy': 0.685,\n",
              "  'val_f1': 0.8130563798219584,\n",
              "  'train_time_sec': 26.72392702102661,\n",
              "  'gpu_mem_mb': 5054.01123046875,\n",
              "  'trainable_params': 104450,\n",
              "  'total_params': 109483778,\n",
              "  'history': {'train': [{'epoch': 0.42857142857142855, 'loss': 0.7939},\n",
              "    {'epoch': 0.8571428571428571, 'loss': 0.6613},\n",
              "    {'epoch': 1.2857142857142856, 'loss': 0.5758},\n",
              "    {'epoch': 1.7142857142857144, 'loss': 0.6494},\n",
              "    {'epoch': 2.142857142857143, 'loss': 0.6511},\n",
              "    {'epoch': 2.571428571428571, 'loss': 0.6161},\n",
              "    {'epoch': 3.0, 'loss': 0.65}],\n",
              "   'eval': [{'epoch': 0.42857142857142855,\n",
              "     'eval_loss': 0.6664366126060486,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 0.8571428571428571,\n",
              "     'eval_loss': 0.6481884717941284,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 1.2857142857142856,\n",
              "     'eval_loss': 0.6074501872062683,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 1.7142857142857144,\n",
              "     'eval_loss': 0.6529775857925415,\n",
              "     'eval_accuracy': 0.695,\n",
              "     'eval_f1': 0.7918088737201365},\n",
              "    {'epoch': 2.142857142857143,\n",
              "     'eval_loss': 0.6184054613113403,\n",
              "     'eval_accuracy': 0.72,\n",
              "     'eval_f1': 0.8271604938271605},\n",
              "    {'epoch': 2.571428571428571,\n",
              "     'eval_loss': 0.5938519835472107,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.5964128971099854,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.5964128971099854,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584}]}},\n",
              " {'task': 'mrpc',\n",
              "  'model_name': 'bert-base-uncased',\n",
              "  'peft_method': 'lora',\n",
              "  'val_accuracy': 0.69,\n",
              "  'val_f1': 0.8154761904761905,\n",
              "  'train_time_sec': 27.116196632385254,\n",
              "  'gpu_mem_mb': 6505.8359375,\n",
              "  'trainable_params': 445444,\n",
              "  'total_params': 109927684,\n",
              "  'history': {'train': [{'epoch': 0.42857142857142855, 'loss': 0.7945},\n",
              "    {'epoch': 0.8571428571428571, 'loss': 0.6698},\n",
              "    {'epoch': 1.2857142857142856, 'loss': 0.5888},\n",
              "    {'epoch': 1.7142857142857144, 'loss': 0.6664},\n",
              "    {'epoch': 2.142857142857143, 'loss': 0.713},\n",
              "    {'epoch': 2.571428571428571, 'loss': 0.6798},\n",
              "    {'epoch': 3.0, 'loss': 0.6621}],\n",
              "   'eval': [{'epoch': 0.42857142857142855,\n",
              "     'eval_loss': 0.6808517575263977,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 0.8571428571428571,\n",
              "     'eval_loss': 0.684060275554657,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 1.2857142857142856,\n",
              "     'eval_loss': 0.6181102991104126,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 1.7142857142857144,\n",
              "     'eval_loss': 0.6960600018501282,\n",
              "     'eval_accuracy': 0.485,\n",
              "     'eval_f1': 0.47715736040609136},\n",
              "    {'epoch': 2.142857142857143,\n",
              "     'eval_loss': 0.6939476132392883,\n",
              "     'eval_accuracy': 0.52,\n",
              "     'eval_f1': 0.52},\n",
              "    {'epoch': 2.571428571428571,\n",
              "     'eval_loss': 0.6367719173431396,\n",
              "     'eval_accuracy': 0.725,\n",
              "     'eval_f1': 0.8318042813455657},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.615203857421875,\n",
              "     'eval_accuracy': 0.69,\n",
              "     'eval_f1': 0.8154761904761905},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.615203857421875,\n",
              "     'eval_accuracy': 0.69,\n",
              "     'eval_f1': 0.8154761904761905}]}},\n",
              " {'task': 'mrpc',\n",
              "  'model_name': 'bert-base-uncased',\n",
              "  'peft_method': 'prefix',\n",
              "  'val_accuracy': 0.685,\n",
              "  'val_f1': 0.8130563798219584,\n",
              "  'train_time_sec': 26.86567759513855,\n",
              "  'gpu_mem_mb': 5205.98486328125,\n",
              "  'trainable_params': 371716,\n",
              "  'total_params': 109853956,\n",
              "  'history': {'train': [{'epoch': 0.42857142857142855, 'loss': 0.7824},\n",
              "    {'epoch': 0.8571428571428571, 'loss': 0.6818},\n",
              "    {'epoch': 1.2857142857142856, 'loss': 0.625},\n",
              "    {'epoch': 1.7142857142857144, 'loss': 0.671},\n",
              "    {'epoch': 2.142857142857143, 'loss': 0.6708},\n",
              "    {'epoch': 2.571428571428571, 'loss': 0.6609},\n",
              "    {'epoch': 3.0, 'loss': 0.6599}],\n",
              "   'eval': [{'epoch': 0.42857142857142855,\n",
              "     'eval_loss': 0.683628261089325,\n",
              "     'eval_accuracy': 0.69,\n",
              "     'eval_f1': 0.8154761904761905},\n",
              "    {'epoch': 0.8571428571428571,\n",
              "     'eval_loss': 0.6954988241195679,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 1.2857142857142856,\n",
              "     'eval_loss': 0.6347468495368958,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 1.7142857142857144,\n",
              "     'eval_loss': 0.6619020700454712,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 2.142857142857143,\n",
              "     'eval_loss': 0.6617526412010193,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 2.571428571428571,\n",
              "     'eval_loss': 0.628418505191803,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.6210105419158936,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584},\n",
              "    {'epoch': 3.0,\n",
              "     'eval_loss': 0.6210105419158936,\n",
              "     'eval_accuracy': 0.685,\n",
              "     'eval_f1': 0.8130563798219584}]}}]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Âè™Ë∑ë BERT\n",
        "base_models  = [\"bert-base-uncased\"]\n",
        "tasks        = [\"sst2\", \"mrpc\"]\n",
        "peft_methods = [\"full_ft\", \"bitfit\", \"lora\", \"prefix\"]\n",
        "\n",
        "# ‰∏∫‰∏çÂêåÊñπÊ≥ïËÆæÁΩÆ‰∏çÂêåÂ≠¶‰π†Áéá\n",
        "lr_map = {\n",
        "    \"full_ft\": 2e-5,\n",
        "    \"bitfit\": 1e-3,\n",
        "    \"lora\":   1e-3,\n",
        "    \"prefix\": 1e-3,\n",
        "}\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for model_name in base_models:\n",
        "    for task_name in tasks:\n",
        "        for method in peft_methods:\n",
        "            print(f\"\\n### Running: {model_name} | {task_name} | {method} ###\")\n",
        "\n",
        "            # 1. ÁîüÊàêÂÆûÈ™åÈÖçÁΩÆÔºàËøôÈáåË¶ÜÁõñ learning_rateÔºâ\n",
        "            config = ExperimentConfig(\n",
        "                model_name=model_name,\n",
        "                task_name=task_name,\n",
        "                peft_method=method,\n",
        "                training=TrainingConfig(\n",
        "                    learning_rate=lr_map[method],\n",
        "                    batch_size=128,\n",
        "                    num_epochs=3,\n",
        "                ),\n",
        "                peft=PEFTConfig(),\n",
        "            )\n",
        "\n",
        "            # 2. ÈÄâ builder\n",
        "            builder_fn = get_builder(model_name, method)\n",
        "\n",
        "            # 3. build Ê®°Âûã\n",
        "            if method in [\"lora\", \"prefix\"]:\n",
        "                model, trainable, total = builder_fn(\n",
        "                    model_name, config.peft, num_labels=2\n",
        "                )\n",
        "            else:\n",
        "                model, trainable, total = builder_fn(\n",
        "                    model_name, num_labels=2\n",
        "                )\n",
        "\n",
        "            # ÂêéÈù¢‰øùÊåÅ‰∏çÂèò...\n",
        "            base_metrics_fn = metrics_fn_dict.get(model_name, compute_metrics)\n",
        "            metrics_fn = base_metrics_fn\n",
        "\n",
        "            current_raw_dataset = raw_datasets[task_name]\n",
        "            if not isinstance(current_raw_dataset, DatasetDict):\n",
        "                current_raw_dataset = DatasetDict(current_raw_dataset)\n",
        "\n",
        "            result = run_single_experiment(\n",
        "                config=config,\n",
        "                raw_dataset=current_raw_dataset,\n",
        "                model=model,\n",
        "                compute_metrics_fn=metrics_fn,\n",
        "                trainable_params=trainable,\n",
        "                total_params=total,\n",
        "            )\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "with open(\"./results/all_results.json\", \"w\") as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "all_results\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
